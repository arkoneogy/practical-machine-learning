library(data.table)
library(ggplot2)
library(caret)
training= fread('pml-training.csv')
final.testing= fread('pml-testing.csv')
dim(training)
names(training)
irrel.cols= c('V1','user_name')
rel.cols= setdiff(names(training), irrel.cols)
training= training[, rel.cols, with=F]
# basic preprocessing
na.cols= names(training)[training[, lapply(.SD, function(x){sum(is.na(x))}), .SDcols= names(training)]>0]
non.na.cols= setdiff(names(training), na.cols)
training = training[,non.na.cols,with=F]
nzv= nearZeroVar(training, saveMetrics = T)
nzv.cols= rownames(nzv)[nzv$nzv==T]
non.nzv.cols= setdiff(names(training), nzv.cols)
training = training[, non.nzv.cols, with=F]
dim(training)
names(training)
str(training)
# check principal components
num.cols= names(training)[sapply(training, mode)=='numeric']
pcs= prcomp(training[,num.cols, with=F], center= T, scale.= T)
vars= (pcs$sdev)^2
vars.norm= vars/sum(vars)
plot(cumsum(vars.norm), type='l') # just 20 princomps will do for 90% variance
title('Variance captured vs number of Principal Components')
xlabel('Number of PCAs')
ylabel('Fraction of total variation captured')
plot(cumsum(vars.norm), type='l',
title('Variance captured vs number of Principal Components'),
xlabel('Number of PCAs'),
ylabel('Fraction of total variation captured'))
plot(cumsum(vars.norm), type='l',
title= 'Variance captured vs number of Principal Components',
xlabel= 'Number of PCAs',
ylabel= 'Fraction of total variation captured')
plot(cumsum(vars.norm), type='l',
title= 'Variance captured vs number of Principal Components',
xlab= 'Number of PCAs',
ylab= 'Fraction of total variation captured')
plot(cumsum(vars.norm), type='l',
main= 'Variance captured vs number of Principal Components',
xlab= 'Number of PCAs',
ylab= 'Fraction of total variation captured')
plot(cumsum(vars.norm), type='l',
main= 'Variance captured vs number of Principal Components',
xlab= 'Number of PCAs',
ylab= 'Fraction of variation captured')
library(data.table)
library(ggplot2)
library(caret)
training= fread('pml-training.csv')
final.testing= fread('pml-testing.csv')
dim(training)
names(training)
irrel.cols= c('V1','user_name')
rel.cols= setdiff(names(training), irrel.cols)
training= training[, rel.cols, with=F]
# basic preprocessing
na.cols= names(training)[training[, lapply(.SD, function(x){sum(is.na(x))}), .SDcols= names(training)]>0]
non.na.cols= setdiff(names(training), na.cols)
training = training[,non.na.cols,with=F]
nzv= nearZeroVar(training, saveMetrics = T)
nzv.cols= rownames(nzv)[nzv$nzv==T]
non.nzv.cols= setdiff(names(training), nzv.cols)
training = training[, non.nzv.cols, with=F]
dim(training)
names(training)
str(training)
num.cols= names(training)[sapply(training, mode)=='numeric']
pcs= prcomp(training[,num.cols, with=F], center= T, scale.= T)
vars= (pcs$sdev)^2
vars.norm= vars/sum(vars)
plot(cumsum(vars.norm), type='l',
main= 'Variance captured vs number of Principal Components',
xlab= 'Number of PCs',
ylab= 'Fraction of variation captured')
set.seed(12936)
training[, classe:= as.factor(classe)]
intrain= as.vector(createDataPartition(training$classe, p= 0.7, list=F))
mytrain= training[intrain,]
mytest= training[-intrain,]
ff= as.formula(paste('classe ~', paste(num.cols, collapse= '+')))
library(doMC)
registerDoMC(3)
ctrl.gbm= trainControl(method= 'cv', number= 10, classProbs = TRUE,
preProcOptions = list(thresh= 0.9))
gbm.grid= expand.grid(interaction.depth= c(2,3),
n.trees= c(300,400,500),
shrinkage= 0.05, n.minobsinnode= c(10,20))
gbm.pca= train(ff, data= mytrain, method= 'gbm', preProcess= 'pca',
trControl= ctrl.gbm, tuneGrid= gbm.grid, verbose= F)
library(data.table)
library(ggplot2)
library(caret)
training= fread('pml-training.csv')
final.testing= fread('pml-testing.csv')
View(training)
head(training)
q('no)')
q('no')
